---
title: "RB Stat Predictions"
author: "Jack Motta"
format: 
  html:
    embed-resources: true
execute: 
  warning: false
editor: visual
---

## Load Libraries

```{r}
library(nflreadr)
library(nflfastR)
library(tidyverse)
library(tidymodels)
library(zoo)
library(gt)
library(doParallel)
library(glmnet)
library(factoextra)
library(cluster)
library(foreach)
```

## Data Acquisition

### Running Backs

```{r}
options(nflreadr.verbose=FALSE) # Hide nflverse messages

# Load base RB stats
rb_stats <- load_player_stats(
  2017:most_recent_season(F), stat_type = "offense") %>%
  filter(position == "RB") %>%
  rename(team = recent_team) %>%
  rename(recent_team = team) %>%
  select(-contains("passing"), -contains("pass"), -attempts, -completions,
         -contains("sack"), -interceptions, -dakota, -pacr, -wopr, -special_teams_tds,
         -contains("fumbles"))

# Create game_info Contextual Data at the Game Level
cl <- makeCluster(7)  
registerDoParallel(cl)
game_info <- load_pbp(2017:most_recent_season(F)) %>%
  group_by(game_id, season, week) %>%
  summarize(
    # Use distinct game-level attributes
    home_team = first(home_team),
    away_team = first(away_team),
    game_date = first(game_date),
    .groups = "drop"
  )
stopCluster(cl)
registerDoSEQ()

# Duplicate rows to have game_id for each team
game_info <- game_info %>%
  select(game_id, game_date, season, week, home_team, away_team) %>%
  mutate(recent_team = home_team) %>%
  bind_rows(
    game_info %>%
      select(game_id, game_date, season, week, home_team, away_team) %>%
      mutate(recent_team = away_team)
  ) %>%
  arrange(game_date, season, week, game_id)

# Load NextGen Rush Stats
nextgen_rush <- load_nextgen_stats(2017:most_recent_season(F), stat_type = "rushing") %>%
  filter(player_position == "RB") %>%
  filter(week != 0) %>%
  select(-player_jersey_number, -player_gsis_id, -player_last_name, 
         -player_first_name, -player_short_name, -player_position, 
         -rush_attempts, -rush_yards, -rush_touchdowns, -contains("receiving"), 
         -contains("expected")) %>%
  rename(recent_team = team_abbr)

# Pull Active RBs
active_rbs <- load_rosters_weekly(2017:most_recent_season(F)) %>%
  filter(position == "RB") %>%  # Filter for RBs
  filter(season == most_recent_season(F), status == "ACT") %>% # Filter active QBs
  pull(full_name)

adv_rush <- load_pfr_advstats(2018:most_recent_season(F), stat_type = "rush", summary_level = "week") %>%
  rename(player_display_name = pfr_player_name,
         recent_team = team) %>%
  select(-pfr_game_id, -game_type, -opponent, -carries, -pfr_player_id,
         -contains("receiving"))

adv_rec <- load_pfr_advstats(2018:most_recent_season(F), stat_type = "rec", summary_level = "week") %>%
  select(-contains("rushing"), -contains("passing"), -pfr_player_id, -game_type,
         -pfr_game_id) %>%
  rename(player_display_name = pfr_player_name,
         recent_team = team,
         opponent_team = opponent) %>%
  mutate(across(player_display_name, clean_player_names))

# Join Base Stats with NextGen Stats and Game Info
rb_stats <- rb_stats %>%
  left_join(nextgen_rush, by = c("recent_team", "player_display_name", "season", 
                                  "season_type", "week")) %>%
  left_join(game_info, by = c("season", "week", "recent_team")) %>%
  left_join(adv_rush, by = c("player_display_name", "recent_team", "season", "week", "game_id")) %>%
  left_join(adv_rec, by = c("player_display_name", "recent_team", "season", "week", "game_id",
                            "opponent_team")) %>%
  filter(player_display_name %in% active_rbs)

colSums(is.na(rb_stats)) # Check NAs

# Create binary home/away flag
rb_stats <- rb_stats %>%
  # Create home/away indicator
  mutate(
    home_away = case_when(
      recent_team == away_team ~ "Away",
      recent_team == home_team ~ "Home",
      TRUE ~ NA_character_
    )) %>%
  # Rearrange columns
  select(-headshot_url, -position, -position_group, -player_name, -home_team, 
         -away_team, -player_id) %>%
  # Make rushing_epa NAs 0
  mutate(rushing_epa = ifelse(is.na(rushing_epa), 0, rushing_epa),
         across(player_display_name, clean_player_names)) %>%
  select(player_display_name, recent_team, season, week, game_date, game_id,
         opponent_team, season_type, home_away, everything())

colSums(is.na(rb_stats)) # Check remaining NAs

rm(active_rbs, nextgen_rush, adv_rush, adv_rec, game_info)
gc()
```

### Opposing Defenses

```{r}
# Load weekly defensive player stats
defense <- load_player_stats(2017:most_recent_season(F), "defense") %>%
  select(-season_type, -player_id, -position_group, -position, -headshot_url, -player_name) %>%
  rename(recent_team = team)

adv_def <- load_pfr_advstats(2018:most_recent_season(F), stat_type = "def",
                             summary_level = "week") %>%
  select(-def_ints, -def_sacks, -pfr_game_id, -game_type,
         -pfr_player_id, -opponent) %>%
  rename(recent_team = team,
         player_display_name = pfr_player_name) %>%
  mutate(across(player_display_name, clean_player_names))
  

# Create opponent defense stats dataframe grouped by team, week, and season
opponent_defense_stats <- rb_stats %>%
  group_by(opponent_team, week, season) %>%
  summarise(
    def_fantasy_points_allowed = first(fantasy_points),
    def_fantasy_points_ppr_allowed = first(fantasy_points_ppr),
    def_carries_allowed = first(carries),
    def_rushing_yards_allowed = first(rushing_yards), # Rushing yards allowed
    def_rushing_tds_allowed = first(rushing_tds), # Rushing TDs allowed
    def_rac_allowed = first(receiving_yards_after_catch),
    def_receiving_tds_allowed = first(receiving_tds),
    def_receiving_yards_allowed = first(receiving_yards),
    def_receptions_allowed = first(receptions),
    .groups = "drop"
  )

defense <- defense %>%
  left_join(adv_def, by = c("recent_team", "week", "season", "player_display_name")) %>%
  # Exclude specific columns from summation
  select(-player_display_name) %>%
  group_by(recent_team, season, week) %>%
  summarise(
    across(where(is.numeric), sum, .names = "{col}", na.rm = TRUE), 
    .groups = "drop") %>%
  arrange(season, week, recent_team) %>%
  rename(opponent_team = recent_team) %>%
  inner_join(opponent_defense_stats, by = c("opponent_team", "season", "week"))

defense <- defense %>%
  select(-contains('def_fumble'), -def_interception_yards,
         -contains("sack"), -def_safety)

colSums(is.na(defense))
```

### Merging

```{r}
gamelogs <- rb_stats %>%
  left_join(defense, by = c("opponent_team", "week", "season"))
arrow::write_parquet(gamelogs, "rb_gamelogs_pre.parquet")

rm(rb_stats, defense, adv_def, opponent_defense_stats, cl)
gc()
```

### Rolling Averages

```{r}
gamelogs <- arrow::read_parquet("rb_gamelogs_pre.parquet")

gamelogs <- gamelogs %>%
  arrange(season, week) %>%  # Ensure correct order for rolling calculations
  group_by(player_display_name) %>%
  mutate(across(where(is.numeric) & !starts_with("def") & !all_of(c("season", "week")), 
                ~ lag(rollapply(.x, width=4, 
                  FUN=function(x) {
                    alpha <- 2 / (length(x) + 1)  # Compute smoothing factor
                    Reduce(function(prev, curr) alpha * curr + (1 - alpha) * prev, x, accumulate = TRUE)[length(x)]
                    },
                  fill=mean(.x, na.rm=TRUE), align="right", partial=TRUE), 1),  # Exclude current row using lag
                .names = "roll_{.col}")) %>%
  ungroup() %>%
  group_by(opponent_team) %>%
  mutate(across(starts_with("def_"), 
                ~ lag(rollapply(.x, width=4, 
                  FUN=function(x) {
                    alpha <- 2 / (length(x) + 1)  # Compute smoothing factor
                    Reduce(function(prev, curr) alpha * curr + (1 - alpha) * prev, x, accumulate = TRUE)[length(x)]
                    },
                  fill=mean(.x, na.rm=TRUE), align="right", partial=TRUE), 1),  # Exclude current row for defense stats too
                .names = "roll_{.col}")) %>%
  ungroup() %>%
  select(player_display_name, recent_team, season, week, season_type, 
         game_id, game_date, home_away, opponent_team, carries, rushing_yards, 
         rushing_tds, receptions, receiving_yards, fantasy_points, 
         fantasy_points_ppr, starts_with("roll_")) %>%
  arrange(game_date, season, week, player_display_name) %>%
  filter(season >= 2018) %>%
  filter(carries > 0)

sum(is.na(gamelogs)) # Total NAs in gamelogs
head(gamelogs) # View first few rows of gamelogs

arrow::write_parquet(gamelogs, "rb_gamelogs_post.parquet")
```

## Modeling - Classification
```{r}
set.seed(6341) # Set seed for reproducibility

gamelogs <- arrow::read_parquet("rb_gamelogs_post.parquet") %>%
  mutate(
    carries_bin              = cut(carries, breaks = c(-Inf, 5, 10, 15, Inf), labels = 0:3, right = FALSE),
    rushing_yards_bin        = cut(rushing_yards, breaks = c(-Inf, 20, 50, 100, Inf), labels = 0:3, right = FALSE),
    rushing_tds_bin          = cut(rushing_tds, breaks = c(0, 1, 2, Inf), labels = 0:2, right = FALSE),
    fantasy_points_bin       = cut(fantasy_points, breaks = c(-Inf, 10, 15, 25, Inf), labels = 0:3, right = FALSE),
    fantasy_points_ppr_bin   = cut(fantasy_points_ppr, breaks = c(-Inf, 10, 20, 30, Inf), labels = 0:3, right = FALSE),
    receptions_bin           = cut(receptions, breaks = c(-Inf, 1, 3, 6, Inf), labels = 0:3, right = FALSE),
    receiving_yards_bin      = cut(receiving_yards, breaks = c(-Inf, 10, 25, 50, Inf), labels = 0:3, right = FALSE)
  ) %>%
  mutate(across(ends_with("_bin"), ~ as.integer(as.character(.))))


# Create a single split for the dataset
player_splits <- make_splits(
  gamelogs %>% filter(season < most_recent_season(F)),
  gamelogs %>% filter(season >= most_recent_season(F))
)

# Extract training and testing datasets
player_train <- training(player_splits)
player_test <- testing(player_splits)
```

### Preprocessing Recipe
```{r}
id_vars <- c("player_display_name", "recent_team", "opponent_team", "season", "week", "game_id", "game_date")

reg_target_vars <- c("carries", "rushing_yards", "rushing_tds", "fantasy_points",
                 "fantasy_points_ppr", "receptions", "receiving_yards")

target_class_vars <- c("carries_bin", "rushing_yards_bin", "rushing_tds_bin", "fantasy_points_bin",
                 "fantasy_points_ppr_bin", "receptions_bin", "receiving_yards_bin")

nfl_recipe <- recipe(~., data = player_train) %>%    
  # Update roles
  step_rm(all_of(reg_target_vars)) %>%
  update_role(all_of(target_class_vars), new_role = "outcome") %>%
  update_role(all_of(id_vars), new_role = "id") %>%
  # Impute Missing Values
  step_impute_mode(all_nominal_predictors()) %>%
  step_impute_mean(all_numeric_predictors()) %>%
  # Handle categorical data
  step_novel(all_nominal_predictors()) %>%
  step_unknown(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors(), -all_outcomes(), one_hot = TRUE) %>%
  # Remove near-zero variance features
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors())

rec_prep <- prep(nfl_recipe)
player_baked <- bake(rec_prep, new_data = NULL)

rm(gamelogs)
gc()
```
### Training
```{r}
# Assume player_baked is already sorted by time
x_mat <- model.matrix(~ ., player_baked %>% select(-all_of(c(target_class_vars, id_vars))))[, -1]
n <- nrow(x_mat)

num_folds <- 10
assess_window <- floor(n * 0.1)
skip <- floor((n - floor(n * 0.3) - assess_window) / num_folds)
initial_window <- n - (num_folds * skip + assess_window)

rolling_splits <- rolling_origin(
  data.frame(idx = 1:n),
  initial = initial_window,
  assess = assess_window,
  skip = skip,
  cumulative = FALSE
)

lambda_grid <- 10^seq(0.1, -5, length.out = 100)
min_class_size <- 8

# Store best lambdas and final models
best_lambdas <- list()
all_models <- list()

cl <- makeCluster(10)
registerDoParallel(cl)

results <- foreach(target = target_class_vars, .packages = c("glmnet", "rsample")) %dopar% {
  y <- player_baked[[target]]
  n <- nrow(x_mat)

  assess_window <- floor(n * 0.1)
  num_folds <- 10
  skip <- floor((n - floor(n * 0.3) - assess_window) / num_folds)
  initial_window <- n - (num_folds * skip + assess_window)

  rolling_splits <- rsample::rolling_origin(
    data.frame(idx = 1:n),
    initial = initial_window,
    assess = assess_window,
    skip = skip,
    cumulative = FALSE
  )

  lambda_logloss <- matrix(NA, nrow = length(lambda_grid), ncol = length(rolling_splits$splits))

  for (i in seq_along(rolling_splits$splits)) {
    split <- rolling_splits$splits[[i]]
    train_idx <- analysis(split)$idx
    val_idx <- assessment(split)$idx

    train_y <- y[train_idx]
    class_counts <- table(train_y)

    if (any(class_counts < min_class_size)) {
      next
    }

    fit <- glmnet::glmnet(
      x = x_mat[train_idx, ],
      y = train_y,
      alpha = 1,
      family = "multinomial",
      lambda = lambda_grid
    )

    preds <- predict(fit, newx = x_mat[val_idx, , drop = FALSE], type = "response")

    for (j in seq_along(lambda_grid)) {
      prob_mat <- preds[, , j]
      true <- factor(y[val_idx])
      pred_probs <- prob_mat[cbind(1:nrow(prob_mat), as.numeric(true))]
      log_loss <- -mean(log(pmax(pred_probs, 1e-15)))
      lambda_logloss[j, i] <- log_loss
    }
  }

  avg_logloss <- rowMeans(lambda_logloss, na.rm = TRUE)

  if (all(is.na(avg_logloss))) {
    return(NULL)
  }

  best_idx <- which.min(avg_logloss)
  best_lambda <- lambda_grid[best_idx]

  final_model <- glmnet::glmnet(
    x = x_mat,
    y = y,
    alpha = 1,
    family = "multinomial",
    lambda = best_lambda
  )

  list(target = target, model = final_model, lambda = best_lambda)
}

stopCluster(cl)
registerDoSEQ()

results <- purrr::compact(results)  # remove any NULLs
all_models <- setNames(purrr::map(results, "model"), purrr::map_chr(results, "target"))
saveRDS(all_models, "prob_model_rb.rds")
```
### Model Evaluation
```{r, warning=FALSE, message=FALSE}
# Predict on test set
player_test_baked <- bake(rec_prep, new_data = player_test)

x_test_mat <- model.matrix(~ ., player_test_baked %>% select(-all_of(c(target_class_vars, id_vars))))[, -1]
y_test <- player_test_baked %>% select(all_of(target_class_vars)) %>% as.data.frame()

# Get predictions for each target
y_pred <- map_dfc(
  .x = target_class_vars,
  .f = ~ {
    pred <- predict(all_models[[.x]], newx = x_test_mat, type = "class")[, 1]
    tibble(!!paste0(.x) := as.integer(pred))
  }
)

# Metric function (classification version)
get_metrics <- function(actual, predicted) {
  levels_union <- sort(unique(c(actual, predicted)))
  actual <- factor(actual, levels = levels_union)
  predicted <- factor(predicted, levels = levels_union)
  acc <- mean(predicted == actual)
  f1_weighted <- suppressWarnings(
    yardstick::f_meas_vec(actual, predicted, estimator = "macro_weighted")
  )
  precision_weighted <- suppressWarnings(
    yardstick::precision_vec(actual, predicted, estimator = "macro_weighted")
  )
  recall_weighted <- suppressWarnings(
    yardstick::recall_vec(actual, predicted, estimator = "macro_weighted")
  )
  c(accuracy = acc,
    f1_weighted = f1_weighted,
    precision_weighted = precision_weighted,
    recall_weighted = recall_weighted)
}
# Metrics per response
metrics_df <- purrr::map_dfr(
  .x = target_class_vars,
  .f = ~ {
    m <- get_metrics(y_test[[.x]], y_pred[[.x]])
    tibble(
      response = .x,
      accuracy = m["accuracy"],
      f1_weighted = m["f1_weighted"],
      precision_weighted = m["precision_weighted"],
      recall_weighted = m["recall_weighted"]
    )
  }
) %>%
  mutate(across(where(is.numeric), round, 3))

metrics_df
arrow::write_parquet(metrics_df, "prob_metrics_rb.parquet")
```
### Add Class Probabilities
```{r}
bin_breaks_list <- list(
  carries_bin             = c(-Inf, 5, 10, 15, Inf),
  rushing_yards_bin       = c(-Inf, 20, 50, 100, Inf),
  rushing_tds_bin         = c(0, 1, 2, Inf),
  fantasy_points_bin      = c(-Inf, 10, 15, 25, Inf),
  fantasy_points_ppr_bin  = c(-Inf, 10, 20, 30, Inf),
  receptions_bin          = c(-Inf, 1, 3, 6, Inf),
  receiving_yards_bin     = c(-Inf, 10, 25, 50, Inf)
)

format_range <- function(lower, upper) {
  if (is.infinite(upper)) {
    return(paste0(lower, "_plus"))
  } else if (is.infinite(lower)) {
    return(paste0("0_", upper))
  } else {
    return(paste0(lower, "_", upper))
  }
}

get_class_probs <- function(model, newx, target_name, breaks) {
  probs <- predict(model, newx = newx, type = "response")[, , 1]
  probs_df <- as.data.frame(probs)
  # Use class labels learned by glmnet
  model_labels <- model$classnames
  # Generate human-readable bin ranges just for those learned
  labels <- purrr::map_chr(seq_along(model_labels), function(i) {
    bin_index <- as.integer(model_labels[i]) + 1  # +1 to index into breaks
    format_range(breaks[bin_index], breaks[bin_index + 1])
  })
  colnames(probs_df) <- paste0(target_name, "_", labels, "_prob")
  return(probs_df)
}

# TRAINING PROBABILITIES
train_prob_feats <- purrr::map2_dfc(
  .x = target_class_vars,
  .y = bin_breaks_list[target_class_vars],
  ~ get_class_probs(
      model = all_models[[.x]],
      newx = x_mat,
      target_name = .x,
      breaks = .y
    )
)

# TESTING PROBABILITIES
test_prob_feats <- purrr::map2_dfc(
  .x = target_class_vars,
  .y = bin_breaks_list[target_class_vars],
  ~ get_class_probs(
      model = all_models[[.x]],
      newx = x_test_mat,
      target_name = .x,
      breaks = .y
    )
)

names(train_prob_feats)
combined_prob_feats <- bind_rows(train_prob_feats, test_prob_feats)

gamelogs <- arrow::read_parquet("rb_gamelogs_post.parquet")
gamelogs <- bind_cols(gamelogs, combined_prob_feats)
arrow::write_parquet(gamelogs, "rb_gamelogs_post_class.parquet")

gc()
```

## Modeling - Regression

### Data Split

```{r}
set.seed(6341) # Set seed for reproducibility
gamelogs <- arrow::read_parquet("rb_gamelogs_post_class.parquet") # Efficient Loading

# Create a single split for the dataset
player_splits <- make_splits(
  gamelogs %>% filter(season < most_recent_season(F)),
  gamelogs %>% filter(season >= most_recent_season(F))
)

# Extract training and testing datasets
player_train <- training(player_splits)
player_test <- testing(player_splits)

rm(gamelogs)
gc()
```

### Preprocessing Recipe

```{r}
id_vars <- c("player_display_name", "recent_team", "opponent_team", "season", "week", "game_id", "game_date")

target_vars <- c("carries", "rushing_yards", "rushing_tds", "fantasy_points",
                 "fantasy_points_ppr", "receptions", "receiving_yards")

nfl_recipe <- recipe(~., data = player_train) %>%    
  # Update roles
  update_role(all_of(target_vars), new_role = "outcome") %>%
  update_role(all_of(id_vars), new_role = "id") %>%
  # Impute Missing Values
  step_impute_mode(all_nominal_predictors()) %>%
  step_impute_mean(all_numeric_predictors()) %>%
  # Handle categorical data
  step_novel(all_nominal_predictors()) %>%
  step_unknown(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors(), -all_outcomes(), one_hot = FALSE) %>%
  # Remove near-zero variance features
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors())

rec_prep <- prep(nfl_recipe)
player_baked <- bake(rec_prep, new_data = NULL)
```

### Hyperparameter Tuning

```{r}
set.seed(6341)
# Assume player_baked is sorted by time
x_mat <- model.matrix(~ ., player_baked %>% select(-all_of(c(target_vars, id_vars))))[, -1]
y_mat <- as.matrix(player_baked %>% select(all_of(target_vars)))

n <- nrow(x_mat)

# Setup rolling origin resampling with exactly 10 folds
num_folds <- 10
assess_window <- floor(n * 0.1)
skip <- floor((n - floor(n * 0.3) - assess_window) / num_folds)
initial_window <- n - (num_folds * skip + assess_window)

rolling_splits <- rolling_origin(
  data.frame(idx = 1:n),
  initial = initial_window,
  assess = assess_window,
  skip = skip,
  cumulative = FALSE
)

lambda_grid <- 10^seq(2, -4, length.out = 500)
lambda_rmse <- matrix(NA, nrow = length(lambda_grid), ncol = length(rolling_splits$splits))

# Register 10 cores
cl <- makeCluster(10)
registerDoParallel(cl)

# Preallocate result matrix
lambda_rmse <- matrix(NA, nrow = length(lambda_grid), ncol = length(rolling_splits$splits))

# Parallel loop
results <- foreach(i = seq_along(rolling_splits$splits),
                   .packages = c("glmnet", "rsample")) %dopar% {
  split <- rolling_splits$splits[[i]]
  train_idx <- analysis(split)$idx
  val_idx <- assessment(split)$idx
  
  if (length(train_idx) == 0 || length(val_idx) == 0) {
    return(rep(NA, length(lambda_grid)))
  }
  
  fit <- glmnet(
    x = x_mat[train_idx, ],
    y = y_mat[train_idx, ],
    alpha = 1,
    family = "mgaussian",
    lambda = lambda_grid
  )
  
  preds <- predict(fit, newx = x_mat[val_idx, , drop = FALSE])
  
  rmse_fold <- numeric(length(lambda_grid))
  for (j in seq_along(lambda_grid)) {
    pred_mat <- preds[, , j]
    rmse_vals <- sqrt(colMeans((y_mat[val_idx, ] - pred_mat)^2))
    rmse_fold[j] <- mean(rmse_vals)
  }
  return(rmse_fold)
}

# Convert list of results to matrix
lambda_rmse <- do.call(cbind, results)

# Stop cluster
stopCluster(cl)
registerDoSEQ()

# Compute average RMSE across folds
avg_rmse <- rowMeans(lambda_rmse, na.rm = TRUE)
best_idx <- which.min(avg_rmse)
best_lambda <- lambda_grid[best_idx]

# Compute SE only at best lambda
se_best <- sd(lambda_rmse[best_idx, ], na.rm = TRUE) / sqrt(sum(!is.na(lambda_rmse[best_idx, ])))
lambda_1se <- max(lambda_grid[avg_rmse <= avg_rmse[best_idx] + se_best])
```

#### Plot Tuning

```{r, warning=FALSE, message=FALSE}
plot(log10(lambda_grid), avg_rmse, type = "l", col = "blue", lwd = 2,
     xlab = "log10(lambda)", ylab = "Avg RMSE", main = "Lambda Tuning Curve")
abline(v = log10(best_lambda), col = "red", lty = 2)
abline(v = log10(lambda_1se), col = "green", lty = 2)
legend("topright", legend = c("Best", "1SE"), col = c("red", "green"), lty = 2)
```

### Refit Model

```{r}
set.seed(6341)
final_fit <- glmnet(
  x = x_mat,
  y = y_mat,
  alpha = 1,
  family = "mgaussian",
  lambda = best_lambda
)  # ready for prediction

saveRDS(final_fit, "RB_Pred_Model.rds")
```

### Evaluate on Test Set

```{r}
# Bake test set using same recipe
player_test_baked <- bake(rec_prep, new_data = player_test)

x_test_mat <- model.matrix(~ ., player_test_baked %>% select(-all_of(c(target_vars, id_vars))))[, -1]
y_test <- as.matrix(player_test_baked %>% select(all_of(target_vars)))

# Predict
y_pred <- predict(final_fit, newx = x_test_mat)[, , 1]

# Metric function
get_metrics <- function(actual, predicted) {
  rmse <- sqrt(mean((actual - predicted)^2))
  rsq <- 1 - sum((actual - predicted)^2) / sum((actual - mean(actual))^2)
  c(rmse = rmse, rsq = rsq)
}

# Metrics per response
metrics_df <- purrr::map_dfr(
  .x = colnames(y_test),
  .f = ~ {
    m <- get_metrics(y_test[, .x], y_pred[, .x])
    tibble(response = .x, rmse = m["rmse"], rsq = m["rsq"])
  }
) %>%
  mutate(across(where(is.numeric), round, 2))

metrics_df
arrow::write_parquet(metrics_df, "RB_Metrics.parquet")
coef(final_fit)
```

## Create Final Prediction Dataframe

```{r}
# Make sure id_vars are still in the baked test set
id_df <- player_test %>% select(all_of(id_vars), -game_id)

# Convert predictions to tibble
pred_df <- as_tibble(y_pred)
colnames(pred_df) <- paste0("pred_", colnames(y_pred))

# Combine original IDs + predictions
pred_output <- bind_cols(id_df, pred_df)
pred_output <- pred_output %>%
  mutate(across(where(is.numeric), ~ round(.x, 1)))
pred_output

arrow::write_parquet(pred_output, "RB_Preds.parquet")
```
