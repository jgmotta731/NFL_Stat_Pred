---
title: "QB Stat Predictions"
author: "Jack Motta"
format: 
  html:
    embed-resources: true
execute: 
  warning: false
editor: visual
---

```{r, warning=FALSE, message=FALSE}
library(nflreadr)
library(nflfastR)
library(tidyverse)
library(tidymodels)
library(zoo)
library(gt)
library(doParallel)
library(glmnet)
library(factoextra)
library(cluster)
library(purrr)
library(tibble)
library(foreach)
```

# Quarterbacks

## Data Acquisition

```{r, warning=FALSE, message=FALSE}
options(nflreadr.verbose=FALSE) # Hide nflverse messages

# Load base QB stats
qb_stats <- load_player_stats(
  2015:most_recent_season(F), stat_type = "offense") %>%
  filter(position == "QB") %>%
  rename(team = recent_team) %>%
  select(-contains("rec"), -player_name) %>%
  mutate(fumbles = sack_fumbles + rushing_fumbles) %>%
  rename(recent_team = team)

# Create game_info Contextual Data at the Game Level
cl <- makeCluster(7)  
registerDoParallel(cl)

game_info <- load_pbp(2015:most_recent_season(F)) %>%
  group_by(game_id, season, week) %>%
  summarize(
    # Use distinct game-level attributes
    home_team = first(home_team),
    away_team = first(away_team),
    game_date = first(game_date),
  ) %>%
  ungroup()

stopCluster(cl)
registerDoSEQ()

# Duplicate rows to have game_id for each team
game_info <- game_info %>%
  select(game_id, game_date, season, week, home_team, away_team) %>%
  mutate(recent_team = home_team) %>%
  bind_rows(
    game_info %>%
      select(game_id, game_date, season, week, home_team, away_team) %>%
      mutate(recent_team = away_team)
  ) %>%
  arrange(game_date, season, week, game_id)

# Load NextGen Passing Stats
nextgen_pass <- load_nextgen_stats(2016:most_recent_season(F), stat_type = "passing") %>%
  filter(player_position == "QB") %>%
  filter(week != 0) %>%
  select(-completions, -pass_yards, -attempts, -player_jersey_number, 
         -player_gsis_id, -pass_touchdowns, -interceptions, -player_last_name,
         -player_first_name, -player_short_name, -player_position) %>%
  rename(recent_team = team_abbr) %>%
  mutate(across(player_display_name, clean_player_names),
         across(recent_team, clean_team_abbrs))

# QBR Data
espn_qbr <- load_espn_qbr(2015:most_recent_season(F), summary_type = "week") %>%
  mutate(
    game_week = case_when(
      week_text == "Wild Card" & season < 2021 ~ 18,
      week_text == "Wild Card" & season >= 2021 ~ 19,
      
      week_text == "Divisional Round" & season < 2021 ~ 19,
      week_text == "Divisional Round" & season >= 2021 ~ 20,
      
      week_text == "Conference Championship" & season < 2021 ~ 20,
      week_text == "Conference Championship" & season >= 2021 ~ 21,
      
      week_text == "Super Bowl" & season < 2021 ~ 21,
      week_text == "Super Bowl" & season >= 2021 ~ 22,
      
      TRUE ~ game_week  # fallback to existing value for regular season
    )
  ) %>%
  select(season, game_week, team_abb, name_display, qbr_total,
         pts_added, qb_plays, epa_total)

espn_qbr <- espn_qbr %>%
  left_join(read.csv("qbr_missing_cleaned.csv"), 
            by = c("season", "game_week"="week", "team_abb",
                   "qbr_total", "pts_added", "qb_plays", "epa_total", "name_display")) %>%
  select(where(~ !all(is.na(.)))) %>%
  mutate(across(name_display, clean_player_names),
         across(team_abb, clean_team_abbrs)) 


# Pull Active QBs
active_qbs <- load_rosters_weekly(2015:most_recent_season(F)) %>%
  filter(position == "QB") %>%  # Filter for quarterbacks
  filter(season == most_recent_season(F)) %>% # Filter active QBs
  filter(status == "ACT") %>%
  distinct(full_name) %>%
  pull(full_name)

# Advanced Pass
adv_pass <- load_pfr_advstats(2018:most_recent_season(F), stat_type = "pass", summary_level = "week") %>%
  select(-contains("rushing"), -contains("receiving"), -pfr_player_id, -game_type, -contains("def_"),
         -pfr_game_id) %>%
  rename(player_display_name = pfr_player_name,
         recent_team = team,
         opponent_team = opponent) %>%
  mutate(across(player_display_name, clean_player_names))

# Join Base Stats with NextGen Stats and Game Info
qb_stats <- qb_stats %>%
  left_join(nextgen_pass, by = c("recent_team", "player_display_name", "season", 
                                  "season_type", "week")) %>%
  left_join(game_info, by = c("season", "week", "recent_team")) %>%
  left_join(espn_qbr, by = c("season", "week"="game_week", 
                             "recent_team"="team_abb", 
                             "player_display_name"="name_display")) %>%
  left_join(adv_pass, by = c("game_id", "player_display_name", "opponent_team",
                             "recent_team", "season", "week")) %>%
  filter(player_display_name %in% active_qbs)

cat("Number of NAs:", sum(is.na(qb_stats))) # Check NAs

# Create binary home/away flag
qb_stats <- qb_stats %>%
  # Create home/away indicator
  mutate(
    home_away = case_when(
      recent_team == away_team ~ "Away",
      recent_team == home_team ~ "Home",
      TRUE ~ NA_character_
    )) %>%
  # Rearrange columns
  select(-fumbles, -racr, -target_share, -wopr, 
         -targets, -special_teams_tds, -fantasy_points_ppr, -headshot_url, -position,
         -position_group, -contains('sack_'), -passing_2pt_conversions,
         -rushing_2pt_conversions, -home_team, -away_team, -air_yards_share,
         -player_id) %>%
  # Make rushing_epa NAs 0
  mutate(rushing_epa = ifelse(is.na(rushing_epa), 0, rushing_epa),
         across(player_display_name, clean_player_names)) %>%
  select(player_display_name, recent_team, season, week, game_date, game_id,
         opponent_team, season_type, home_away, everything())

cat("Remaining NAs:", sum(is.na(qb_stats))) # Check remaining NAs

#| output: false
rm(active_qbs, nextgen_pass, adv_pass, game_info, espn_qbr)
gc()
```

### Opposing Defense

```{r, warning=FALSE, message=FALSE}
# Load weekly defensive player stats
defense <- load_player_stats(2015:most_recent_season(F), "defense") %>%
  select(-season_type, -player_id, -position_group, -position, -headshot_url, -player_name) %>%
  rename(recent_team = team)

adv_def <- load_pfr_advstats(2018:most_recent_season(F), stat_type = "def",
                             summary_level = "week") %>%
  select(-def_ints, -def_sacks, -pfr_game_id, -game_type,
         -pfr_player_id, -opponent) %>%
  rename(recent_team = team,
         player_display_name = pfr_player_name) %>%
  mutate(across(player_display_name, clean_player_names))
  

# Create opponent defense stats dataframe grouped by team, week, and season
opponent_defense_stats <- qb_stats %>%
  group_by(opponent_team, week, season) %>%
  summarise(
    def_attempts_allowed = first(attempts),
    def_carries_allowed = first(carries),
    def_fantasy_points_allowed = first(fantasy_points),
    def_passing_yards_allowed = first(passing_yards), # Passing yards allowed
    def_passing_tds_allowed = first(passing_tds), # Passing TDs allowed
    def_rushing_tds_allowed = first(rushing_tds), # Rushing TDs allowed
    def_completions_allowed = first(completions) # Completions allowed
  ) %>%
  ungroup()

defense <- defense %>%
  left_join(adv_def, by = c("recent_team", "week", "season", "player_display_name")) %>%
  # Exclude specific columns from summation
  select(-player_display_name) %>%
  group_by(recent_team, season, week) %>%
  summarise(
    across(where(is.numeric), sum, .names = "{col}", na.rm = TRUE), 
    .groups = "drop") %>%
  arrange(season, week, recent_team) %>%
  ungroup() %>%
  rename(opponent_team = recent_team) %>%
  inner_join(opponent_defense_stats, by = c("opponent_team", "season", "week"))

defense <- defense %>%
  select(-contains('def_fumble_r'), -def_interception_yards,
         -def_sack_yards, -def_fumbles, -def_safety)

cat("NAs:", sum(is.na(defense)))
```

### Join Offense and Defense Gamelogs

```{r, warning=FALSE, message=FALSE}
gamelogs <- qb_stats %>%
  left_join(defense, by = c("opponent_team", "week", "season"))

#| output: false
rm(qb_stats, defense, adv_def, opponent_defense_stats, cl)
gc()

arrow::write_parquet(gamelogs, "qb_gamelogs_pre.parquet")
```

### Rolling Averages

```{r, warning=FALSE, message=FALSE}
gamelogs <- arrow::read_parquet("qb_gamelogs_pre.parquet")

gamelogs <- gamelogs %>%
  arrange(season, week) %>%  # Ensure correct order for rolling calculations
  group_by(player_display_name) %>%
  mutate(across(where(is.numeric) & !starts_with("def") & !all_of(c("season", "week")), 
                ~ lag(rollapply(.x, width=4, 
                  FUN=function(x) {
                    alpha <- 2 / (length(x) + 1)  # Compute smoothing factor
                    Reduce(function(prev, curr) alpha * curr + (1 - alpha) * prev, x, accumulate = TRUE)[length(x)]
                    },
                  fill=mean(.x, na.rm=TRUE), align="right", partial=TRUE), 1),  # Exclude current row using lag
                .names = "roll_{.col}")) %>%
  ungroup() %>%
  group_by(opponent_team) %>%
  mutate(across(starts_with("def_"), 
                ~ lag(rollapply(.x, width=4, 
                  FUN=function(x) {
                    alpha <- 2 / (length(x) + 1)  # Compute smoothing factor
                    Reduce(function(prev, curr) alpha * curr + (1 - alpha) * prev, x, accumulate = TRUE)[length(x)]
                    },
                  fill=mean(.x, na.rm=TRUE), align="right", partial=TRUE), 1),  # Exclude current row for defense stats too
                .names = "roll_{.col}")) %>%
  ungroup() %>%
  select(player_display_name, recent_team, season, week, season_type, 
         game_id, game_date, home_away, opponent_team, attempts, completions,
         passing_yards, passing_tds, interceptions, carries, rushing_yards, rushing_tds, 
         fantasy_points, starts_with("roll_")) %>%
  arrange(game_date, season, week, player_display_name) %>%
  filter(season >= 2018) %>%
  filter(attempts >= 15)

cat("Dimensions:", dim(gamelogs))
cat("Total NAs:", sum(is.na(gamelogs))) # Total NAs in gamelogs
head(gamelogs) # View first few rows of gamelogs

arrow::write_parquet(gamelogs, "qb_gamelogs_post.parquet")
```

## Modeling - Classification
```{r}
set.seed(6341) # Set seed for reproducibility

gamelogs <- arrow::read_parquet("qb_gamelogs_post.parquet") %>%
  mutate(
    attempts_bin       = cut(attempts, breaks = c(-Inf, 20, 30, 40, Inf), labels = 0:3, right = FALSE),
    completions_bin    = cut(completions, breaks = c(-Inf, 15, 25, 30, Inf), labels = 0:3, right = FALSE),
    passing_yards_bin  = cut(passing_yards, breaks = c(-Inf, 100, 200, 300, Inf), labels = 0:3, right = FALSE),
    passing_tds_bin    = cut(passing_tds, breaks = c(-Inf, 1, 2, 3, Inf), labels = 0:3, right = FALSE),
    interceptions_bin  = cut(interceptions, breaks = c(-Inf, 0, 1, 2, Inf), labels = 0:3, right = FALSE),
    carries_bin        = cut(carries, breaks = c(-Inf, 2, 5, 10, Inf), labels = 0:3, right = FALSE),
    rushing_yards_bin  = cut(rushing_yards, breaks = c(-Inf, 10, 25, 50, Inf), labels = 0:3, right = FALSE),
    rushing_tds_bin    = cut(rushing_tds, breaks = c(0, 1, 2, Inf), labels = 0:2, right = FALSE),
    fantasy_points_bin = cut(fantasy_points, breaks = c(-Inf, 10, 20, 30, Inf), labels = 0:3, right = FALSE)
  ) %>%
  mutate(across(ends_with("_bin"), ~ as.integer(as.character(.))))

# Create a single split for the dataset
player_splits <- make_splits(
  gamelogs %>% filter(season < most_recent_season(F)),
  gamelogs %>% filter(season >= most_recent_season(F))
)

# Extract training and testing datasets
player_train <- training(player_splits)
player_test <- testing(player_splits)
```

### Preprocessing Recipe
```{r}
id_vars <- c("player_display_name", "recent_team", "season", "week", 
             "game_id", "game_date", "opponent_team")

reg_target_vars <- c("attempts", "completions", "passing_yards", "passing_tds", 
                 "interceptions","carries", "rushing_yards", "rushing_tds", 
                 "fantasy_points")

target_class_vars <- c("attempts_bin", "completions_bin", "passing_yards_bin", "passing_tds_bin", 
                 "interceptions_bin","carries_bin", "rushing_yards_bin", "rushing_tds_bin", 
                 "fantasy_points_bin")

nfl_recipe <- recipe(~., data = player_train) %>%    
  # Update roles
  step_rm(all_of(reg_target_vars)) %>%
  update_role(all_of(target_class_vars), new_role = "outcome") %>%
  update_role(all_of(id_vars), new_role = "id") %>%
  # Impute Missing Values
  step_impute_mode(all_nominal_predictors()) %>%
  step_impute_mean(all_numeric_predictors()) %>%
  # Handle categorical data
  step_novel(all_nominal_predictors()) %>%
  step_unknown(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors(), -all_outcomes(), one_hot = TRUE) %>%
  # Remove near-zero variance features
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors())

rec_prep <- prep(nfl_recipe)
player_baked <- bake(rec_prep, new_data = NULL)

#| output: false
rm(gamelogs)
gc()
```
### Training
```{r}
# Assume player_baked is already sorted by time
x_mat <- model.matrix(~ ., player_baked %>% select(-all_of(c(target_class_vars, id_vars))))[, -1]
n <- nrow(x_mat)

num_folds <- 10
assess_window <- floor(n * 0.1)
skip <- floor((n - floor(n * 0.3) - assess_window) / num_folds)
initial_window <- n - (num_folds * skip + assess_window)

rolling_splits <- rolling_origin(
  data.frame(idx = 1:n),
  initial = initial_window,
  assess = assess_window,
  skip = skip,
  cumulative = FALSE
)

lambda_grid <- 10^seq(0.1, -5, length.out = 500)
min_class_size <- 8

# Store best lambdas and final models
best_lambdas <- list()
all_models <- list()

for (target in target_class_vars) {
  y <- player_baked[[target]]
  n <- nrow(x_mat)

  # Define rolling origin splits
  assess_window <- floor(n * 0.1)
  num_folds <- 10
  skip <- floor((n - floor(n * 0.3) - assess_window) / num_folds)
  initial_window <- n - (num_folds * skip + assess_window)

  rolling_splits <- rsample::rolling_origin(
    data.frame(idx = 1:n),
    initial = initial_window,
    assess = assess_window,
    skip = skip,
    cumulative = FALSE
  )

  lambda_logloss <- matrix(NA, nrow = length(lambda_grid), ncol = length(rolling_splits$splits))

  for (i in seq_along(rolling_splits$splits)) {
    split <- rolling_splits$splits[[i]]
    train_idx <- rsample::analysis(split)$idx
    val_idx <- rsample::assessment(split)$idx

    train_y <- y[train_idx]
    class_counts <- table(train_y)

    # Skip fold if any class has < min_class_size
    if (any(class_counts < min_class_size)) {
      warning(paste("Skipping fold", i, "for", target, "- sparse class found"))
      next
    }

    fit <- glmnet::glmnet(
      x = x_mat[train_idx, ],
      y = train_y,
      alpha = 1,
      family = "multinomial",
      lambda = lambda_grid
    )

    preds <- predict(fit, newx = x_mat[val_idx, , drop = FALSE], type = "response")

    for (j in seq_along(lambda_grid)) {
      prob_mat <- preds[, , j]
      true <- factor(y[val_idx])
      pred_probs <- prob_mat[cbind(1:nrow(prob_mat), as.numeric(true))]
      log_loss <- -mean(log(pmax(pred_probs, 1e-15)))  # avoid log(0)
      lambda_logloss[j, i] <- log_loss
    }
  }

  avg_logloss <- rowMeans(lambda_logloss, na.rm = TRUE)

  if (all(is.na(avg_logloss))) {
    warning(paste("No valid folds found for", target, "- skipping model"))
    next
  }

  best_idx <- which.min(avg_logloss)
  best_lambda <- lambda_grid[best_idx]

  best_lambdas[[target]] <- best_lambda

  # Refit final model using best lambda
  final_model <- glmnet::glmnet(
    x = x_mat,
    y = y,
    alpha = 1,
    family = "multinomial",
    lambda = best_lambda
  )

  all_models[[target]] <- final_model
}

saveRDS(final_model, "prob_model.rds")
```
### Model Evaluation
```{r, warning=FALSE, message=FALSE}
# Predict on test set
player_test_baked <- bake(rec_prep, new_data = player_test)

x_test_mat <- model.matrix(~ ., player_test_baked %>% select(-all_of(c(target_class_vars, id_vars))))[, -1]
y_test <- player_test_baked %>% select(all_of(target_class_vars)) %>% as.data.frame()

# Get predictions for each target
y_pred <- map_dfc(
  .x = target_class_vars,
  .f = ~ {
    pred <- predict(all_models[[.x]], newx = x_test_mat, type = "class")[, 1]
    tibble(!!paste0(.x) := as.integer(pred))
  }
)

# Metric function (classification version)
get_metrics <- function(actual, predicted) {
  levels_union <- sort(unique(c(actual, predicted)))
  actual <- factor(actual, levels = levels_union)
  predicted <- factor(predicted, levels = levels_union)
  acc <- mean(predicted == actual)
  f1_weighted <- suppressWarnings(
    yardstick::f_meas_vec(actual, predicted, estimator = "macro_weighted")
  )
  precision_weighted <- suppressWarnings(
    yardstick::precision_vec(actual, predicted, estimator = "macro_weighted")
  )
  recall_weighted <- suppressWarnings(
    yardstick::recall_vec(actual, predicted, estimator = "macro_weighted")
  )
  c(accuracy = acc,
    f1_weighted = f1_weighted,
    precision_weighted = precision_weighted,
    recall_weighted = recall_weighted)
}
# Metrics per response
metrics_df <- purrr::map_dfr(
  .x = target_class_vars,
  .f = ~ {
    m <- get_metrics(y_test[[.x]], y_pred[[.x]])
    tibble(
      response = .x,
      accuracy = m["accuracy"],
      f1_weighted = m["f1_weighted"],
      precision_weighted = m["precision_weighted"],
      recall_weighted = m["recall_weighted"]
    )
  }
) %>%
  mutate(across(where(is.numeric), round, 3))

metrics_df
arrow::write_parquet(metrics_df, "prob_metrics.parquet")
```
### Add Class Probabilities
```{r}
bin_breaks_list <- list(
  attempts_bin       = c(-Inf, 20, 30, 40, Inf),
  completions_bin    = c(-Inf, 15, 20, 30, Inf),
  passing_yards_bin  = c(-Inf, 100, 200, 300, Inf),
  passing_tds_bin    = c(-Inf, 1, 2, 3, Inf),
  interceptions_bin  = c(-Inf, 0, 1, 2, Inf),
  carries_bin        = c(-Inf, 2, 5, 10, Inf),
  rushing_yards_bin  = c(-Inf, 10, 25, 50, Inf),
  rushing_tds_bin    = c(0, 1, 2, Inf),
  fantasy_points_bin = c(-Inf, 10, 20, 30, Inf)
)

format_range <- function(lower, upper) {
  if (is.infinite(upper)) {
    return(paste0(lower, "_plus"))
  } else if (is.infinite(lower)) {
    return(paste0("0_", upper))
  } else {
    return(paste0(lower, "_", upper))
  }
}

get_class_probs <- function(model, newx, target_name, breaks) {
  probs <- predict(model, newx = newx, type = "response")[, , 1]
  probs_df <- as.data.frame(probs)
  # Use class labels learned by glmnet
  model_labels <- model$classnames
  # Generate human-readable bin ranges just for those learned
  labels <- purrr::map_chr(seq_along(model_labels), function(i) {
    bin_index <- as.integer(model_labels[i]) + 1  # +1 to index into breaks
    format_range(breaks[bin_index], breaks[bin_index + 1])
  })
  colnames(probs_df) <- paste0(target_name, "_", labels, "_prob")
  return(probs_df)
}

# TRAINING PROBABILITIES
train_prob_feats <- purrr::map2_dfc(
  .x = target_class_vars,
  .y = bin_breaks_list[target_class_vars],
  ~ get_class_probs(
      model = all_models[[.x]],
      newx = x_mat,
      target_name = .x,
      breaks = .y
    )
)

# TESTING PROBABILITIES
test_prob_feats <- purrr::map2_dfc(
  .x = target_class_vars,
  .y = bin_breaks_list[target_class_vars],
  ~ get_class_probs(
      model = all_models[[.x]],
      newx = x_test_mat,
      target_name = .x,
      breaks = .y
    )
)

names(train_prob_feats)
combined_prob_feats <- bind_rows(train_prob_feats, test_prob_feats)

gamelogs <- arrow::read_parquet("qb_gamelogs_post.parquet")
gamelogs <- bind_cols(gamelogs, combined_prob_feats)
arrow::write_parquet(gamelogs, "qb_gamelogs_post_class.parquet")

rm(gamelogs, fit, final_model, all_models, lambda_grid, metrics_df, bin_breaks_list, train_prob_feats, test_prob_feats, x_test_mat, y_test, y_pred, prob_mat, x_mat, best_idx, best_lambda, best_lambdas, min_class_size, pred_probs, preds, train_y, train_idx, var, y, true, target_class_vars, skip, j, log_loss, i, class_counts, bin_vars, avg_logloss, assess_window, lambda_logloss, combined_prob_feats)
gc()
```

## Modeling - Regression

### Data Split

```{r, warning=FALSE, message=FALSE}
set.seed(6341) # Set seed for reproducibility
gamelogs <- arrow::read_parquet("qb_gamelogs_post_class.parquet")

# Create a single split for the dataset
player_splits <- make_splits(
  gamelogs %>% filter(season < most_recent_season(F)),
  gamelogs %>% filter(season >= most_recent_season(F))
)

# Extract training and testing datasets
player_train <- training(player_splits)
player_test <- testing(player_splits)
```

### Preprocessing Recipe

```{r}
id_vars <- c("player_display_name", "recent_team", "season", "week", 
             "game_id", "game_date", "opponent_team")

target_vars <- c("attempts", "completions", "passing_yards", "passing_tds", 
                 "interceptions","carries", "rushing_yards", "rushing_tds", 
                 "fantasy_points")

nfl_recipe <- recipe(~., data = player_train) %>%    
  # Update roles
  update_role(all_of(target_vars), new_role = "outcome") %>%
  update_role(all_of(id_vars), new_role = "id") %>%
  # Impute Missing Values
  step_impute_mode(all_nominal_predictors()) %>%
  step_impute_mean(all_numeric_predictors()) %>%
  # Handle categorical data
  step_novel(all_nominal_predictors()) %>%
  step_unknown(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors(), -all_outcomes(), one_hot = TRUE) %>%
  # Remove near-zero variance features
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors())

rec_prep <- prep(nfl_recipe)
player_baked <- bake(rec_prep, new_data = NULL)

rm(gamelogs)
gc()
```

### Hyperparameter Tuning

```{r}
set.seed(6341)
# Assume player_baked is sorted by time
x_mat <- model.matrix(~ ., player_baked %>% select(-all_of(c(target_vars, id_vars))))[, -1]
y_mat <- as.matrix(player_baked %>% select(all_of(target_vars)))

n <- nrow(x_mat)

# Setup rolling origin resampling with exactly 10 folds
num_folds <- 10
assess_window <- floor(n * 0.1)
skip <- floor((n - floor(n * 0.3) - assess_window) / num_folds)
initial_window <- n - (num_folds * skip + assess_window)

rolling_splits <- rolling_origin(
  data.frame(idx = 1:n),
  initial = initial_window,
  assess = assess_window,
  skip = skip,
  cumulative = FALSE
)

lambda_grid <- 10^seq(0.1, -5, length.out = 100)
lambda_rmse <- matrix(NA, nrow = length(lambda_grid), ncol = length(rolling_splits$splits))

# Register 10 cores
cl <- makeCluster(10)
registerDoParallel(cl)

# Preallocate result matrix
lambda_rmse <- matrix(NA, nrow = length(lambda_grid), ncol = length(rolling_splits$splits))

# Parallel loop
results <- foreach(i = seq_along(rolling_splits$splits),
                   .packages = c("glmnet", "rsample")) %dopar% {
  split <- rolling_splits$splits[[i]]
  train_idx <- analysis(split)$idx
  val_idx <- assessment(split)$idx
  
  if (length(train_idx) == 0 || length(val_idx) == 0) {
    return(rep(NA, length(lambda_grid)))
  }
  
  fit <- glmnet(
    x = x_mat[train_idx, ],
    y = y_mat[train_idx, ],
    alpha = 1,
    family = "mgaussian",
    lambda = lambda_grid
  )
  
  preds <- predict(fit, newx = x_mat[val_idx, , drop = FALSE])
  
  rmse_fold <- numeric(length(lambda_grid))
  for (j in seq_along(lambda_grid)) {
    pred_mat <- preds[, , j]
    rmse_vals <- sqrt(colMeans((y_mat[val_idx, ] - pred_mat)^2))
    rmse_fold[j] <- mean(rmse_vals)
  }
  return(rmse_fold)
}

# Convert list of results to matrix
lambda_rmse <- do.call(cbind, results)

# Stop cluster
stopCluster(cl)
registerDoSEQ()

# Compute average RMSE across folds
avg_rmse <- rowMeans(lambda_rmse, na.rm = TRUE)
best_idx <- which.min(avg_rmse)
best_lambda <- lambda_grid[best_idx]

# Compute SE only at best lambda
se_best <- sd(lambda_rmse[best_idx, ], na.rm = TRUE) / sqrt(sum(!is.na(lambda_rmse[best_idx, ])))
lambda_1se <- max(lambda_grid[avg_rmse <= avg_rmse[best_idx] + se_best])
```

#### Plot Tuning

```{r, warning=FALSE}
plot(log10(lambda_grid), avg_rmse, type = "l", col = "blue", lwd = 2,
     xlab = "log10(lambda)", ylab = "Avg RMSE", main = "Lambda Tuning Curve")
abline(v = log10(best_lambda), col = "red", lty = 2)
abline(v = log10(lambda_1se), col = "green", lty = 2)
legend("topright", legend = c("Best", "1SE"), col = c("red", "green"), lty = 2)
```

### Refit Model with Best Lambda

```{r}
set.seed(6341)
final_fit <- glmnet(
  x = x_mat,
  y = y_mat,
  alpha = 1,
  family = "mgaussian",
  lambda = best_lambda
)

saveRDS(final_fit, "QB_Pred_Model.rds")
```

### Evaluate on Test Set

```{r}
# Bake test set using same recipe
player_test_baked <- bake(rec_prep, new_data = player_test)

x_test_mat <- model.matrix(~ ., player_test_baked %>% select(-all_of(c(target_vars, id_vars))))[, -1]
y_test <- as.matrix(player_test_baked %>% select(all_of(target_vars)))

# Predict
y_pred <- predict(final_fit, newx = x_test_mat)[, , 1]

# Metric function
get_metrics <- function(actual, predicted) {
  rmse <- sqrt(mean((actual - predicted)^2))
  rsq <- 1 - sum((actual - predicted)^2) / sum((actual - mean(actual))^2)
  c(rmse = rmse, rsq = rsq)
}

# Metrics per response
metrics_df <- purrr::map_dfr(
  .x = colnames(y_test),
  .f = ~ {
    m <- get_metrics(y_test[, .x], y_pred[, .x])
    tibble(response = .x, rmse = m["rmse"], rsq = m["rsq"])
  }
) %>%
  mutate(across(where(is.numeric), round, 2))

metrics_df
arrow::write_parquet(metrics_df, "QB_Metrics.parquet")
coef(final_fit)
```

## Extract Predictions

```{r}
# Make sure id_vars are still in the baked test set
id_df <- player_test %>% select(all_of(id_vars))

# Convert predictions to tibble
pred_df <- as_tibble(y_pred)
colnames(pred_df) <- paste0("pred_", colnames(y_pred))

# Combine original IDs + predictions
pred_output <- bind_cols(id_df, pred_df)
pred_output <- pred_output %>%
  mutate(across(where(is.numeric), ~ round(.x, 1)))
pred_output

arrow::write_parquet(pred_output, "QB_Preds.parquet")
```